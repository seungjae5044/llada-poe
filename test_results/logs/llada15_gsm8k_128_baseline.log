The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-10-24:02:12:53 INFO     [__main__:440] Selected Tasks: ['gsm8k']
2025-10-24:02:12:53 WARNING  [evaluator:163] Model appears to be an instruct variant but chat template is not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).
2025-10-24:02:12:53 INFO     [evaluator:189] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-10-24:02:12:53 INFO     [evaluator:227] Initializing llada_dist model, with arguments: {'model_path': 'GSAI-ML/LLaDA-8B-Instruct', 'gen_length': 128, 'steps': 128, 'block_length': 32, 'show_speed': True}
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 178.53it/s]
2025-10-24:02:13:04 INFO     [evaluator:290] gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}
2025-10-24:02:13:04 WARNING  [evaluator:309] Overwriting default num_fewshot of gsm8k from 5 to 5
2025-10-24:02:13:04 INFO     [api.task:434] Building contexts for gsm8k on rank 0...
  0%|          | 0/1319 [00:00<?, ?it/s]  2%|▏         | 31/1319 [00:00<00:04, 304.23it/s]  5%|▍         | 63/1319 [00:00<00:04, 312.37it/s]  7%|▋         | 95/1319 [00:00<00:03, 314.41it/s] 10%|▉         | 127/1319 [00:00<00:03, 316.08it/s] 12%|█▏        | 159/1319 [00:00<00:03, 317.17it/s] 14%|█▍        | 191/1319 [00:00<00:03, 317.23it/s] 17%|█▋        | 223/1319 [00:00<00:03, 318.05it/s] 19%|█▉        | 255/1319 [00:00<00:03, 318.62it/s] 22%|██▏       | 287/1319 [00:00<00:03, 318.88it/s] 24%|██▍       | 319/1319 [00:01<00:03, 318.84it/s] 27%|██▋       | 352/1319 [00:01<00:03, 319.28it/s] 29%|██▉       | 384/1319 [00:01<00:02, 318.85it/s] 32%|███▏      | 416/1319 [00:01<00:02, 318.33it/s] 34%|███▍      | 448/1319 [00:01<00:02, 317.92it/s] 36%|███▋      | 480/1319 [00:01<00:02, 317.38it/s] 39%|███▉      | 512/1319 [00:01<00:02, 314.65it/s] 41%|████      | 544/1319 [00:01<00:02, 311.85it/s] 44%|████▎     | 576/1319 [00:01<00:02, 314.02it/s] 46%|████▌     | 608/1319 [00:01<00:02, 315.49it/s] 49%|████▊     | 640/1319 [00:02<00:02, 316.22it/s] 51%|█████     | 672/1319 [00:02<00:02, 316.79it/s] 53%|█████▎    | 704/1319 [00:02<00:01, 317.08it/s] 56%|█████▌    | 736/1319 [00:02<00:01, 317.63it/s] 58%|█████▊    | 768/1319 [00:02<00:01, 317.86it/s] 61%|██████    | 801/1319 [00:02<00:01, 318.78it/s] 63%|██████▎   | 833/1319 [00:02<00:01, 315.72it/s] 66%|██████▌   | 866/1319 [00:02<00:01, 317.11it/s] 68%|██████▊   | 899/1319 [00:02<00:01, 318.57it/s] 71%|███████   | 931/1319 [00:02<00:01, 318.93it/s] 73%|███████▎  | 964/1319 [00:03<00:01, 319.55it/s] 76%|███████▌  | 997/1319 [00:03<00:01, 319.93it/s] 78%|███████▊  | 1029/1319 [00:03<00:00, 319.25it/s] 80%|████████  | 1061/1319 [00:03<00:00, 310.00it/s] 83%|████████▎ | 1093/1319 [00:03<00:00, 312.85it/s] 85%|████████▌ | 1125/1319 [00:03<00:00, 314.12it/s] 88%|████████▊ | 1157/1319 [00:03<00:00, 308.97it/s] 90%|█████████ | 1188/1319 [00:03<00:00, 303.78it/s] 92%|█████████▏| 1219/1319 [00:03<00:00, 299.48it/s] 95%|█████████▍| 1249/1319 [00:03<00:00, 296.45it/s] 97%|█████████▋| 1280/1319 [00:04<00:00, 300.05it/s]100%|█████████▉| 1313/1319 [00:04<00:00, 306.43it/s]100%|██████████| 1319/1319 [00:04<00:00, 314.02it/s]
2025-10-24:02:13:08 INFO     [evaluator:559] Running generate_until requests
Batching...:   0%|          | 0/1319 [00:00<?, ?it/s]Batching...: 100%|██████████| 1319/1319 [00:00<00:00, 1899823.82it/s]
Generating...:   0%|          | 0/1319 [00:00<?, ?it/s]